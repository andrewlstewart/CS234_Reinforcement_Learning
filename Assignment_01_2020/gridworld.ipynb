{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gridworld\n",
    "\n",
    "###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![Gridworld](imgs/Q1_Gridworld.PNG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Givens:\n",
    "\n",
    "$\\gamma=1$\n",
    "\n",
    "$r_s\\in \\{-1, 0, 1\\}$\n",
    "\n",
    "$r_{5} = -5$\n",
    "\n",
    "$r_{12} = 5$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1"
   ]
  },
  {
   "source": [
    "![a)](imgs/Q1_a.PNG)\n",
    "### a)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$r_s >= 0$ will not necessarily find the shortest path.  $r_s < 0$ will.\n",
    "\n",
    "Optimal value of state 1 = $r_t + \\gamma r_{(t+1)} + \\gamma^2 r_{(t+2)} + ...$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((16, 1)) - 1\n",
    "R[11] = 5\n",
    "R[4] = -5"
   ]
  },
  {
   "source": [
    "One set of transitions which will return the shortest path are:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros((16,16))\n",
    "P[0, 1] = 1\n",
    "P[1, 2] = 1\n",
    "P[2, 3] = 1\n",
    "P[3, 7] = 1\n",
    "# P[4, 1] = 1\n",
    "P[5, 6] = 1\n",
    "P[6, 7] = 1\n",
    "P[7, 11] = 1\n",
    "P[8, 9] = 1\n",
    "P[9, 10] = 1\n",
    "P[10, 11] = 1\n",
    "# P[11, ]\n",
    "P[12, 8] = 1\n",
    "P[13, 12] = 1\n",
    "P[14, 13] = 1\n",
    "P[15, 14] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.matmul(np.linalg.inv(np.eye(16) - gamma * P), R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [-5.,  2.,  3.,  4.],\n",
       "       [ 2.,  3.,  4.,  5.],\n",
       "       [ 1.,  0., -1., -2.]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "V.reshape((4,4))  # Reshaped to the gridworld for easier viewing "
   ]
  },
  {
   "source": [
    "![b)](imgs/Q1_b.PNG)\n",
    "### b)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "R += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [-3.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 7.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.matmul(np.linalg.inv(np.eye(16) - gamma * P), R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[12., 11., 10.,  9.],\n",
       "       [-3., 10.,  9.,  8.],\n",
       "       [10.,  9.,  8.,  7.],\n",
       "       [11., 12., 13., 14.]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "V.reshape((4,4))"
   ]
  },
  {
   "source": [
    "![c)](imgs/Q1_c.PNG)\n",
    "### c)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Old value is the discounted sum of future rewards\n",
    "\n",
    "$V_{old}^\\pi=\\mathbb{E}_\\pi\\left[\\sum_{T=0}^{\\infty}\\gamma^Tr_{t+T}|s_t=s\\right]$\n",
    "\n",
    "New value is the old value + constant\n",
    "\n",
    "$V_{new}^\\pi=\\mathbb{E}_\\pi\\left[\\sum_{T=0}^{\\infty}\\gamma^T\\left(r_{t+T}+c\\right)|s_t=s\\right]$\n",
    "\n",
    "$V_{new}^\\pi=\\mathbb{E}_\\pi\\left[\\sum_{T=0}^{\\infty}\\gamma^Tr_{t+T}+\\gamma^Tc|s_t=s\\right]$\n",
    "\n",
    "$V_{new}^\\pi=\\mathbb{E}_\\pi\\left[\\sum_{T=0}^{\\infty}\\gamma^Tr_{t+T}|s_t=s\\right] + \\sum_{T=0}^{\\infty}\\gamma^Tc$\n",
    "\n",
    "For $\\gamma < 1$, the infinite sum is equal to $c/(1-\\gamma)$\n",
    "\n",
    "$V_{new}^\\pi=V_{old} + \\frac{c}{1-\\gamma}$\n",
    "\n",
    "Note, for our example above, gamma did not fit that infinite sum criteria"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![d)](imgs/Q1_d.PNG)\n",
    "### d)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[12., 11., 10.,  9.],\n",
       "       [-3., 10.,  9.,  8.],\n",
       "       [10.,  9.,  8.,  7.],\n",
       "       [11., 12., 13., 14.]])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "V.reshape((4,4))"
   ]
  },
  {
   "source": [
    "Because an agent would receive positive rewards in many states, for an infinite horizon problem, the agent would move around randomly while avoiding the states with terminal conditions.  The value of the unshaded squares will approach $+\\infty$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![e)](imgs/Q1_e.PNG)\n",
    "### e)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For very small values of $\\gamma$, the immediate rewards become much more important.  This may result in non infinite values because the agent may prioritize getting to a terminal state rather than spend infinite time collecting small rewards.  IE. the agent might get 'tricked' into thinkin gthat large immediate rewards is better than infinite small rewards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![f)](imgs/Q1_f.PNG)\n",
    "### f)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The furthest from the $r_g$ while closest to $r_r$ is 3 steps, for $\\gamma=1$, the equivalent value will be when $r_r + r_s = 3r_s + r_g$ or $-5 + r_s = 3r_s + 5$ or $r_s = -5$\n",
    "\n",
    "Therefore, if $r_s<-5$, there will be an optimal policy which terminates at the red shaded area and if $r_s=-5$ then an optimal policy could terminate at the red shaded area or the green shaded area for an agent in square {6, 10, 9, 13, 14, 15, 16}."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}